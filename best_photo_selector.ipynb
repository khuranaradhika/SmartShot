{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0b9c56fb",
      "metadata": {
        "id": "0b9c56fb"
      },
      "source": [
        "# Best‑Photo Selector Pipeline\n",
        "Automatically pick the strongest frame from a burst using a blend of aesthetic, technical, and face‑quality metrics.\n",
        "\n",
        "*Built for quick hackathon demos — tweak as needed!*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dced62f6",
      "metadata": {
        "id": "dced62f6"
      },
      "source": [
        "### How this notebook works\n",
        "1. **Setup** – install packages (run once in your Colab/local runtime).\n",
        "2. **Scoring functions** – sharpness, exposure, face centering, smile, and overall aesthetic.\n",
        "3. **Batch evaluate** any folder of images and return a ranked list.\n",
        "4. **Preview + export** – display the top N images and optionally copy them to a new directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0bf6839f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bf6839f",
        "outputId": "ce2daf98-0e46-42ff-db33-f2c4c30c401c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.6/108.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# 👉 Run this cell first (may take ~2 min in Colab)\n",
        "!pip install -q open-clip-torch==2.20.0 mediapipe opencv-python pillow tqdm deepface\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#rank_folder('/content/drive/MyDrive/bursts', top_k=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uxkm4cqV_Hv3",
        "outputId": "1806491d-16ff-437c-aaf4-aef977ab5602"
      },
      "id": "Uxkm4cqV_Hv3",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "afe285ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afe285ef",
        "outputId": "a90b0a70-516e-48ea-acb6-15782577fc12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import os, cv2, math, torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import mediapipe as mp\n",
        "import open_clip\n",
        "\n",
        "# Device setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "837da096",
      "metadata": {
        "id": "837da096"
      },
      "outputs": [],
      "source": [
        "### ---------- Technical quality scorers ----------\n",
        "def sharpness_score(img):\n",
        "    \"\"\"Variance of Laplacian (normalized 0‑1).\"\"\"\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    score = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "    return score\n",
        "\n",
        "def exposure_score(img):\n",
        "    \"\"\"Penalize over/under‑exposure using histogram clipping.\"\"\"\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    hist = cv2.calcHist([gray],[0],None,[256],[0,256]).flatten()\n",
        "    total = hist.sum()\n",
        "    low_clip = hist[:5].sum()/total\n",
        "    high_clip = hist[-5:].sum()/total\n",
        "    return 1 - (low_clip + high_clip)  # closer to 1 is better\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "894a0968",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "17eab177956a460da6fe082d41288ab3",
            "616abc10904447c2827a3ce9c2448a2b",
            "df847625337d44d289b8bdea9687bfda",
            "0a9e0508575e49b38f0dbd08f8d81e2e",
            "4c93a96a4cf7479fa1f9f78095a55960",
            "4c7e4ef4375b4cc49cbb98e3f9b03063",
            "4595041cadaa467b9e4374ebc4d2f4ba",
            "c81938d3123a4caaa00939bd7914b92d",
            "85194c37d8174aea8848319c8d88d26c",
            "f868169dcea8471d9e891bc8d46513be",
            "ab45a73644c3408bb56c69abbb86eda6"
          ]
        },
        "id": "894a0968",
        "outputId": "129849a5-6944-4d8b-c37e-d84e73c7b753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "open_clip_pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17eab177956a460da6fe082d41288ab3"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "### ---------- Aesthetic score using CLIP ----------\n",
        "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n",
        "        'ViT-B-32', pretrained='laion2b_s34b_b79k', device=device)\n",
        "clip_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_tokens = open_clip.tokenize([\"a beautiful photo\"]).to(device)\n",
        "    text_embed = clip_model.encode_text(text_tokens).float()\n",
        "\n",
        "def aesthetic_score(pil_img):\n",
        "    img = clip_preprocess(pil_img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        img_embed = clip_model.encode_image(img).float()\n",
        "    score = torch.cosine_similarity(img_embed, text_embed).item()\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ↳ run in a notebook code cell\n",
        "!git clone --depth 1 https://github.com/akanametov/yolo-face\n",
        "!pip install -q ultralytics    # YOLO framework (pulls torch, etc.)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDEvMvBfHMJ0",
        "outputId": "43607927-4d3e-4599-eb49-ba16dcd67f5a"
      },
      "id": "GDEvMvBfHMJ0",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolo-face'...\n",
            "remote: Enumerating objects: 331, done.\u001b[K\n",
            "remote: Counting objects: 100% (331/331), done.\u001b[K\n",
            "remote: Compressing objects: 100% (281/281), done.\u001b[K\n",
            "remote: Total 331 (delta 48), reused 227 (delta 43), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (331/331), 35.83 MiB | 27.26 MiB/s, done.\n",
            "Resolving deltas: 100% (48/48), done.\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/akanametov/yolo-face/releases/download/v0.0.0/yolov10n-face.pt\n",
        "from ultralytics import YOLO\n",
        "face_detector = YOLO(\"yolov10n-face.pt\")   # downloads weights once"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvcmExPGHQbT",
        "outputId": "90a5b2c1-7609-4ffb-a567-3970f34f2ce1"
      },
      "id": "QvcmExPGHQbT",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f2e8ab66",
      "metadata": {
        "id": "f2e8ab66"
      },
      "outputs": [],
      "source": [
        "import numpy as np, math, cv2\n",
        "from PIL import Image\n",
        "\n",
        "def face_metrics(pil_img):\n",
        "    \"\"\"\n",
        "    Returns (face_found, centering(0-1), smile_prob(0-1))\n",
        "    \"\"\"\n",
        "    img = np.array(pil_img)\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    # 1. face bbox\n",
        "    res = face_detector(img, imgsz=640, conf=0.25)[0]\n",
        "    if not len(res.boxes):\n",
        "        return 0, 0, 0          # no face detected\n",
        "\n",
        "    # take the biggest box\n",
        "    x1,y1,x2,y2 = res.boxes.xyxy.cpu().numpy().astype(int)[0]\n",
        "    cx, cy = (x1+x2)/2, (y1+y2)/2\n",
        "    centering = 1 - math.hypot(cx-w/2, cy-h/2)/math.hypot(w/2, h/2)\n",
        "\n",
        "    # 2. smile proxy – mouth aspect ratio from landmarks if available\n",
        "    # fall back to face orientation (slight smile raises cheeks)\n",
        "    smile = float(res.boxes.conf[0])      # confidence ≈ “face quality”\n",
        "    return 1, np.clip(centering,0,1), np.clip(smile,0,1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def norm(val, lo, hi):\n",
        "    return np.clip((val-lo)/(hi-lo), 0, 1)\n"
      ],
      "metadata": {
        "id": "DmVQb-Q5J_4-"
      },
      "id": "DmVQb-Q5J_4-",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combined_score(pil_img,\n",
        "                   w_center=0.20, w_smile=0.15,\n",
        "                   w_sharp=0.25, w_exp=0.15, w_aes=0.25):\n",
        "    img_cv = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    sharp_raw = sharpness_score(img_cv)\n",
        "    sharp = norm(sharp_raw, 80, 600)\n",
        "\n",
        "    expo  = exposure_score(img_cv)        # already 0-1\n",
        "    aes   = norm(aesthetic_score(pil_img), .20, .35)\n",
        "\n",
        "    face_found, centering, smile = face_metrics(pil_img)\n",
        "    if not face_found:\n",
        "        centering, smile = 0, 0           # still penalise\n",
        "\n",
        "    total = (w_center*centering + w_smile*smile +\n",
        "             w_sharp*sharp    + w_exp*expo + w_aes*aes)\n",
        "\n",
        "    return total, dict(sharp=sharp, expo=expo, aes=aes,\n",
        "                       center=centering, smile=smile)\n"
      ],
      "metadata": {
        "id": "cvVbv5c2KEHh"
      },
      "id": "cvVbv5c2KEHh",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "11ca9047",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11ca9047",
        "outputId": "f61ea0cc-533b-488d-e374-20ca2d438559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rScoring:   0%|          | 0/8 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 640x480 1 face, 39.9ms\n",
            "Speed: 14.6ms preprocess, 39.9ms inference, 41.9ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rScoring:  12%|█▎        | 1/8 [00:00<00:05,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 640x480 1 face, 12.4ms\n",
            "Speed: 3.5ms preprocess, 12.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rScoring:  25%|██▌       | 2/8 [00:01<00:03,  1.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 640x480 1 face, 8.5ms\n",
            "Speed: 3.5ms preprocess, 8.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rScoring:  38%|███▊      | 3/8 [00:01<00:02,  2.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 640x480 1 face, 8.9ms\n",
            "Speed: 3.6ms preprocess, 8.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rScoring:  50%|█████     | 4/8 [00:02<00:01,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 640x480 1 face, 12.0ms\n",
            "Speed: 4.6ms preprocess, 12.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rScoring:  62%|██████▎   | 5/8 [00:02<00:01,  2.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 640x480 1 face, 8.6ms\n",
            "Speed: 3.6ms preprocess, 8.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rScoring:  75%|███████▌  | 6/8 [00:02<00:00,  2.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 640x480 1 face, 9.5ms\n",
            "Speed: 3.7ms preprocess, 9.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rScoring:  88%|████████▊ | 7/8 [00:03<00:00,  2.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 640x480 1 face, 14.1ms\n",
            "Speed: 4.1ms preprocess, 14.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scoring: 100%|██████████| 8/8 [00:03<00:00,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(np.float64(0.6956167736529094), PosixPath('/content/drive/My Drive/bursts/IMG_1.jpg'), {'sharp': np.float64(0.8139828824678464), 'expo': np.float32(0.8625286), 'aes': np.float64(0.23490152756373087), 'center': np.float64(0.8612610364749685), 'smile': np.float64(0.8784277439117432)}), (np.float64(0.6695943844928872), PosixPath('/content/drive/My Drive/bursts/IMG_3.jpg'), {'sharp': np.float64(0.6733251802790946), 'expo': np.float32(0.87269425), 'aes': np.float64(0.23714296023050943), 'center': np.float64(0.8883028648532112), 'smile': np.float64(0.8894175887107849)}), (np.float64(0.6636774805375835), PosixPath('/content/drive/My Drive/bursts/IMG_5.jpg'), {'sharp': np.float64(0.7672163629774306), 'expo': np.float32(0.8613354), 'aes': np.float64(0.18275392055511472), 'center': np.float64(0.8341800675431328), 'smile': np.float64(0.8676572442054749)}), (np.float64(0.6542383226149172), PosixPath('/content/drive/My Drive/bursts/IMG_2.jpg'), {'sharp': np.float64(0.6235242556855715), 'expo': np.float32(0.92389065), 'aes': np.float64(0.15814256668090818), 'center': np.float64(0.9635822845307397), 'smile': np.float64(0.8501437306404114)}), (np.float64(0.6326468408003786), PosixPath('/content/drive/My Drive/bursts/IMG_6.jpg'), {'sharp': np.float64(0.5996948926548479), 'expo': np.float32(0.84142137), 'aes': np.float64(0.2177642981211344), 'center': np.float64(0.857954232727617), 'smile': np.float64(0.8698532581329346)}), (np.float64(0.6215482070966176), PosixPath('/content/drive/My Drive/bursts/IMG_4.jpg'), {'sharp': np.float64(0.5385561928768271), 'expo': np.float32(0.9036406), 'aes': np.float64(0.19432894388834632), 'center': np.float64(0.862550918518893), 'smile': np.float64(0.8684709072113037)}), (np.float64(0.5687030334973078), PosixPath('/content/drive/My Drive/bursts/IMG_7.jpg'), {'sharp': np.float64(0.2530481082529045), 'expo': np.float32(0.9178755), 'aes': np.float64(0.2578301429748535), 'center': np.float64(0.868151602209204), 'smile': np.float64(0.8644787669181824)}), (np.float64(0.5191216974746313), PosixPath('/content/drive/My Drive/bursts/IMG_8.jpg'), {'sharp': np.float64(0.3942758011321776), 'expo': np.float32(0.9504833), 'aes': np.float64(0.24840784072875974), 'center': np.float64(0.8210852158149026), 'smile': np.float64(0.344408243894577)})]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "### ---------- Evaluate a folder ----------\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "def rank_folder(folder, top_k=3, out_dir=None):\n",
        "    paths = list(Path(folder).glob('*.[jp][pn]g'))  # jpg & png\n",
        "    results = []\n",
        "    for p in tqdm(paths, desc='Scoring'):\n",
        "        img = Image.open(p).convert('RGB')\n",
        "        score, parts = combined_score(img)\n",
        "        results.append((score, p, parts))\n",
        "    results.sort(reverse=True, key=lambda x: x[0])\n",
        "    print(results)\n",
        "    if out_dir:\n",
        "        Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
        "        for _, p, _ in results[:top_k]:\n",
        "            shutil.copy(p, Path(out_dir)/p.name)\n",
        "    return results[:top_k], results\n",
        "\n",
        "# Example:\n",
        "top, all_scores = rank_folder('/content/drive/My Drive/bursts', top_k=5, out_dir='/content/drive/My Drive/burstoutput')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3718fa26",
      "metadata": {
        "id": "3718fa26"
      },
      "source": [
        "*Last updated: 2025‑05‑31 14:42 UTC*"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UL36vKyK8AKh"
      },
      "id": "UL36vKyK8AKh",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "17eab177956a460da6fe082d41288ab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_616abc10904447c2827a3ce9c2448a2b",
              "IPY_MODEL_df847625337d44d289b8bdea9687bfda",
              "IPY_MODEL_0a9e0508575e49b38f0dbd08f8d81e2e"
            ],
            "layout": "IPY_MODEL_4c93a96a4cf7479fa1f9f78095a55960"
          }
        },
        "616abc10904447c2827a3ce9c2448a2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c7e4ef4375b4cc49cbb98e3f9b03063",
            "placeholder": "​",
            "style": "IPY_MODEL_4595041cadaa467b9e4374ebc4d2f4ba",
            "value": "open_clip_pytorch_model.bin: 100%"
          }
        },
        "df847625337d44d289b8bdea9687bfda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c81938d3123a4caaa00939bd7914b92d",
            "max": 605219813,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85194c37d8174aea8848319c8d88d26c",
            "value": 605219813
          }
        },
        "0a9e0508575e49b38f0dbd08f8d81e2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f868169dcea8471d9e891bc8d46513be",
            "placeholder": "​",
            "style": "IPY_MODEL_ab45a73644c3408bb56c69abbb86eda6",
            "value": " 605M/605M [00:02&lt;00:00, 240MB/s]"
          }
        },
        "4c93a96a4cf7479fa1f9f78095a55960": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c7e4ef4375b4cc49cbb98e3f9b03063": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4595041cadaa467b9e4374ebc4d2f4ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c81938d3123a4caaa00939bd7914b92d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85194c37d8174aea8848319c8d88d26c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f868169dcea8471d9e891bc8d46513be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab45a73644c3408bb56c69abbb86eda6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}