{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0b9c56fb",
      "metadata": {
        "id": "0b9c56fb"
      },
      "source": [
        "# Best‑Photo Selector Pipeline\n",
        "Automatically pick the strongest frame from a burst using a blend of aesthetic, technical, and face‑quality metrics.\n",
        "\n",
        "*Built for quick hackathon demos — tweak as needed!*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dced62f6",
      "metadata": {
        "id": "dced62f6"
      },
      "source": [
        "### How this notebook works\n",
        "1. **Setup** – install packages (run once in your Colab/local runtime).\n",
        "2. **Scoring functions** – sharpness, exposure, face centering, smile, and overall aesthetic.\n",
        "3. **Batch evaluate** any folder of images and return a ranked list.\n",
        "4. **Preview + export** – display the top N images and optionally copy them to a new directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bf6839f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bf6839f",
        "outputId": "88c475ae-a157-4efb-be38-9cb6e49f8809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.6/108.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# 👉 Run this cell first (may take ~2 min in Colab)\n",
        "!pip install -q open-clip-torch==2.20.0 mediapipe opencv-python pillow tqdm deepface\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#rank_folder('/content/drive/MyDrive/bursts', top_k=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uxkm4cqV_Hv3",
        "outputId": "4499b7b7-7271-4af8-c64c-0ccae4760e6a"
      },
      "id": "Uxkm4cqV_Hv3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afe285ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afe285ef",
        "outputId": "19dd3319-ff9f-4b6c-e4bb-ebd04e16de71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import os, cv2, math, torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import mediapipe as mp\n",
        "import open_clip\n",
        "\n",
        "# Device setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "837da096",
      "metadata": {
        "id": "837da096"
      },
      "outputs": [],
      "source": [
        "### ---------- Technical quality scorers ----------\n",
        "def sharpness_score(img):\n",
        "    \"\"\"Variance of Laplacian (normalized 0‑1).\"\"\"\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    score = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "    return score\n",
        "\n",
        "def exposure_score(img):\n",
        "    \"\"\"Penalize over/under‑exposure using histogram clipping.\"\"\"\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    hist = cv2.calcHist([gray],[0],None,[256],[0,256]).flatten()\n",
        "    total = hist.sum()\n",
        "    low_clip = hist[:5].sum()/total\n",
        "    high_clip = hist[-5:].sum()/total\n",
        "    return 1 - (low_clip + high_clip)  # closer to 1 is better\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "894a0968",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "2d2b6c0df2d444928fbb38777e5e7cdd",
            "8ee153985e8143b18b924c5e12797cfd",
            "7c3670f123d44a41aeb191c41dee3b60",
            "79759ed249e24a2e802bb7dafd2941f1",
            "4102b6d6c46449089d3a063a586aa524",
            "dd70464b0753492398c24c8a7a675efa",
            "c979af5ea6174d61b8c7de81bd334c05",
            "5220a51ae79947e8a352599df1d8b895",
            "8532881cabeb45d1a98f05162d3e7aeb",
            "15fb9c4201be41dfb05a2d0782cbc789",
            "f22ff0677d8743a4a86d379a49d18058"
          ]
        },
        "id": "894a0968",
        "outputId": "e44e66de-90e2-42b6-8064-fdea1f6ae4c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "open_clip_pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d2b6c0df2d444928fbb38777e5e7cdd"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "### ---------- Aesthetic score using CLIP ----------\n",
        "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n",
        "        'ViT-B-32', pretrained='laion2b_s34b_b79k', device=device)\n",
        "clip_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_tokens = open_clip.tokenize([\"a beautiful photo\"]).to(device)\n",
        "    text_embed = clip_model.encode_text(text_tokens).float()\n",
        "\n",
        "def aesthetic_score(pil_img):\n",
        "    img = clip_preprocess(pil_img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        img_embed = clip_model.encode_image(img).float()\n",
        "    score = torch.cosine_similarity(img_embed, text_embed).item()\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ↳ run in a notebook code cell\n",
        "!git clone --depth 1 https://github.com/akanametov/yolo-face\n",
        "!pip install -q ultralytics    # YOLO framework (pulls torch, etc.)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDEvMvBfHMJ0",
        "outputId": "cd1d4c1f-4769-4ff4-a856-7a7bb0f074e7"
      },
      "id": "GDEvMvBfHMJ0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolo-face'...\n",
            "remote: Enumerating objects: 331, done.\u001b[K\n",
            "remote: Counting objects: 100% (331/331), done.\u001b[K\n",
            "remote: Compressing objects: 100% (281/281), done.\u001b[K\n",
            "remote: Total 331 (delta 48), reused 227 (delta 43), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (331/331), 35.83 MiB | 17.39 MiB/s, done.\n",
            "Resolving deltas: 100% (48/48), done.\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/akanametov/yolo-face/releases/download/v0.0.0/yolov10n-face.pt\n",
        "from ultralytics import YOLO\n",
        "face_detector = YOLO(\"yolov10n-face.pt\")   # downloads weights once"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvcmExPGHQbT",
        "outputId": "86dac772-8b76-4e37-ce78-4e67ea7a269f"
      },
      "id": "QvcmExPGHQbT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2e8ab66",
      "metadata": {
        "id": "f2e8ab66"
      },
      "outputs": [],
      "source": [
        "import numpy as np, math, cv2\n",
        "from PIL import Image\n",
        "\n",
        "def face_metrics(pil_img):\n",
        "    \"\"\"\n",
        "    Returns (face_found, centering(0-1), smile_prob(0-1))\n",
        "    \"\"\"\n",
        "    img = np.array(pil_img)\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    # 1. face bbox\n",
        "    res = face_detector(img, imgsz=640, conf=0.25)[0]\n",
        "    if not len(res.boxes):\n",
        "        return 0, 0, 0          # no face detected\n",
        "\n",
        "    # take the biggest box\n",
        "    x1,y1,x2,y2 = res.boxes.xyxy.cpu().numpy().astype(int)[0]\n",
        "    cx, cy = (x1+x2)/2, (y1+y2)/2\n",
        "    centering = 1 - math.hypot(cx-w/2, cy-h/2)/math.hypot(w/2, h/2)\n",
        "\n",
        "    # 2. smile proxy – mouth aspect ratio from landmarks if available\n",
        "    # fall back to face orientation (slight smile raises cheeks)\n",
        "    smile = float(res.boxes.conf[0])      # confidence ≈ “face quality”\n",
        "    return 1, np.clip(centering,0,1), np.clip(smile,0,1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def norm(val, lo, hi):\n",
        "    return np.clip((val-lo)/(hi-lo), 0, 1)\n"
      ],
      "metadata": {
        "id": "DmVQb-Q5J_4-"
      },
      "id": "DmVQb-Q5J_4-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combined_score(pil_img,\n",
        "                   w_center=0.20, w_smile=0.15,\n",
        "                   w_sharp=0.25, w_exp=0.15, w_aes=0.25):\n",
        "    img_cv = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    sharp_raw = sharpness_score(img_cv)\n",
        "    sharp = norm(sharp_raw, 80, 600)\n",
        "\n",
        "    expo  = exposure_score(img_cv)        # already 0-1\n",
        "    aes   = norm(aesthetic_score(pil_img), .20, .35)\n",
        "\n",
        "    face_found, centering, smile = face_metrics(pil_img)\n",
        "    if not face_found:\n",
        "        centering, smile = 0, 0           # still penalise\n",
        "\n",
        "    total = (w_center*centering + w_smile*smile +\n",
        "             w_sharp*sharp    + w_exp*expo + w_aes*aes)\n",
        "\n",
        "    return total, dict(sharp=sharp, expo=expo, aes=aes,\n",
        "                       center=centering, smile=smile)\n"
      ],
      "metadata": {
        "id": "cvVbv5c2KEHh"
      },
      "id": "cvVbv5c2KEHh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ca9047",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11ca9047",
        "outputId": "988127be-c3b5-41a0-b27f-85e6c8bcd507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rScoring:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 640x480 1 face, 9.1ms\n",
            "Speed: 3.4ms preprocess, 9.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 face, 8.9ms\n",
            "Speed: 3.5ms preprocess, 8.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rScoring:  20%|██        | 2/10 [00:00<00:00,  9.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 640x480 1 face, 8.6ms\n",
            "Speed: 3.3ms preprocess, 8.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 face, 8.7ms\n",
            "Speed: 3.5ms preprocess, 8.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rScoring:  40%|████      | 4/10 [00:00<00:00, 10.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 640x480 1 face, 9.0ms\n",
            "Speed: 3.5ms preprocess, 9.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 face, 11.4ms\n",
            "Speed: 3.9ms preprocess, 11.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rScoring:  60%|██████    | 6/10 [00:00<00:00, 10.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 640x480 1 face, 16.7ms\n",
            "Speed: 3.8ms preprocess, 16.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 face, 15.0ms\n",
            "Speed: 4.0ms preprocess, 15.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rScoring:  80%|████████  | 8/10 [00:00<00:00,  8.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 640x480 1 face, 15.5ms\n",
            "Speed: 3.8ms preprocess, 15.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rScoring:  90%|█████████ | 9/10 [00:01<00:00,  8.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 640x480 1 face, 19.5ms\n",
            "Speed: 4.2ms preprocess, 19.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scoring: 100%|██████████| 10/10 [00:01<00:00,  8.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(np.float64(0.6956167736529094), PosixPath('/content/drive/MyDrive/bursts/IMG-20250531-WA0005.jpg'), {'sharp': np.float64(0.8139828824678464), 'expo': np.float32(0.8625286), 'aes': np.float64(0.23490152756373087), 'center': np.float64(0.8612610364749685), 'smile': np.float64(0.8784277439117432)}), (np.float64(0.6695943844928872), PosixPath('/content/drive/MyDrive/bursts/IMG-20250531-WA0007.jpg'), {'sharp': np.float64(0.6733251802790946), 'expo': np.float32(0.87269425), 'aes': np.float64(0.23714296023050943), 'center': np.float64(0.8883028648532112), 'smile': np.float64(0.8894175887107849)}), (np.float64(0.6636774805375835), PosixPath('/content/drive/MyDrive/bursts/IMG-20250531-WA0006.jpg'), {'sharp': np.float64(0.7672163629774306), 'expo': np.float32(0.8613354), 'aes': np.float64(0.18275392055511472), 'center': np.float64(0.8341800675431328), 'smile': np.float64(0.8676572442054749)}), (np.float64(0.6542383226149172), PosixPath('/content/drive/MyDrive/bursts/IMG-20250531-WA0009.jpg'), {'sharp': np.float64(0.6235242556855715), 'expo': np.float32(0.92389065), 'aes': np.float64(0.15814256668090818), 'center': np.float64(0.9635822845307397), 'smile': np.float64(0.8501437306404114)}), (np.float64(0.6326468408003786), PosixPath('/content/drive/MyDrive/bursts/IMG-20250531-WA0002.jpg'), {'sharp': np.float64(0.5996948926548479), 'expo': np.float32(0.84142137), 'aes': np.float64(0.2177642981211344), 'center': np.float64(0.857954232727617), 'smile': np.float64(0.8698532581329346)}), (np.float64(0.6215482070966176), PosixPath('/content/drive/MyDrive/bursts/IMG-20250531-WA0003.jpg'), {'sharp': np.float64(0.5385561928768271), 'expo': np.float32(0.9036406), 'aes': np.float64(0.19432894388834632), 'center': np.float64(0.862550918518893), 'smile': np.float64(0.8684709072113037)}), (np.float64(0.6124890039998794), PosixPath('/content/drive/MyDrive/bursts/IMG-20250531-WA0004.jpg'), {'sharp': np.float64(0.5710820913757714), 'expo': np.float32(0.9257208), 'aes': np.float64(0.17122250795364377), 'center': np.float64(0.804242496950947), 'smile': np.float64(0.8480415344238281)}), (np.float64(0.5780877991239902), PosixPath('/content/drive/MyDrive/bursts/IMG-20250531-WA0010.jpg'), {'sharp': np.float64(0.460454117493915), 'expo': np.float32(0.9299479), 'aes': np.float64(0.11998317639032995), 'center': np.float64(0.8448186544716151), 'smile': np.float64(0.830150306224823)}), (np.float64(0.5687030334973078), PosixPath('/content/drive/MyDrive/bursts/IMG-20250531-WA0008.jpg'), {'sharp': np.float64(0.2530481082529045), 'expo': np.float32(0.9178755), 'aes': np.float64(0.2578301429748535), 'center': np.float64(0.868151602209204), 'smile': np.float64(0.8644787669181824)}), (np.float64(0.5191216974746313), PosixPath('/content/drive/MyDrive/bursts/IMG-20250531-WA0001.jpg'), {'sharp': np.float64(0.3942758011321776), 'expo': np.float32(0.9504833), 'aes': np.float64(0.24840784072875974), 'center': np.float64(0.8210852158149026), 'smile': np.float64(0.344408243894577)})]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "### ---------- Evaluate a folder ----------\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "def rank_folder(folder, top_k=3, out_dir=None):\n",
        "    paths = list(Path(folder).glob('*.[jp][pn]g'))  # jpg & png\n",
        "    results = []\n",
        "    for p in tqdm(paths, desc='Scoring'):\n",
        "        img = Image.open(p).convert('RGB')\n",
        "        score, parts = combined_score(img)\n",
        "        results.append((score, p, parts))\n",
        "    results.sort(reverse=True, key=lambda x: x[0])\n",
        "    print(results)\n",
        "    if out_dir:\n",
        "        Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
        "        for _, p, _ in results[:top_k]:\n",
        "            shutil.copy(p, Path(out_dir)/p.name)\n",
        "    return results[:top_k], results\n",
        "\n",
        "# Example:\n",
        "top, all_scores = rank_folder('/content/drive/MyDrive/bursts', top_k=5, out_dir='/content/drive/MyDrive/burstoutput')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3718fa26",
      "metadata": {
        "id": "3718fa26"
      },
      "source": [
        "*Last updated: 2025‑05‑31 14:42 UTC*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2d2b6c0df2d444928fbb38777e5e7cdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ee153985e8143b18b924c5e12797cfd",
              "IPY_MODEL_7c3670f123d44a41aeb191c41dee3b60",
              "IPY_MODEL_79759ed249e24a2e802bb7dafd2941f1"
            ],
            "layout": "IPY_MODEL_4102b6d6c46449089d3a063a586aa524"
          }
        },
        "8ee153985e8143b18b924c5e12797cfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd70464b0753492398c24c8a7a675efa",
            "placeholder": "​",
            "style": "IPY_MODEL_c979af5ea6174d61b8c7de81bd334c05",
            "value": "open_clip_pytorch_model.bin: 100%"
          }
        },
        "7c3670f123d44a41aeb191c41dee3b60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5220a51ae79947e8a352599df1d8b895",
            "max": 605219813,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8532881cabeb45d1a98f05162d3e7aeb",
            "value": 605219813
          }
        },
        "79759ed249e24a2e802bb7dafd2941f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15fb9c4201be41dfb05a2d0782cbc789",
            "placeholder": "​",
            "style": "IPY_MODEL_f22ff0677d8743a4a86d379a49d18058",
            "value": " 605M/605M [00:02&lt;00:00, 279MB/s]"
          }
        },
        "4102b6d6c46449089d3a063a586aa524": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd70464b0753492398c24c8a7a675efa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c979af5ea6174d61b8c7de81bd334c05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5220a51ae79947e8a352599df1d8b895": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8532881cabeb45d1a98f05162d3e7aeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15fb9c4201be41dfb05a2d0782cbc789": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f22ff0677d8743a4a86d379a49d18058": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}