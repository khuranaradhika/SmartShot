{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#--- Social Media-Ready Editing ---"
      ],
      "metadata": {
        "id": "Ttv0gL20Ny2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHx-k4NQNtLX"
      },
      "outputs": [],
      "source": [
        "def apply_social_media_edits(pil_image, preferred_aspect_ratio=(4,5)):\n",
        "    \"\"\"Applies common social media edits: crop, enhance, sharpen.\"\"\"\n",
        "    img_width, img_height = pil_image.size\n",
        "\n",
        "    # Smart Cropping (AI for object-aware cropping is a future enhancement)\n",
        "    target_w, target_h = preferred_aspect_ratio\n",
        "\n",
        "    potential_h = int(img_width * (target_h / target_w))\n",
        "    if potential_h <= img_height:\n",
        "        crop_width = img_width\n",
        "        crop_height = potential_h\n",
        "        left = 0\n",
        "        top = (img_height - crop_height) // 2\n",
        "    else:\n",
        "        crop_height = img_height\n",
        "        crop_width = int(img_height * (target_w / target_h))\n",
        "        left = (img_width - crop_width) // 2\n",
        "        top = 0\n",
        "\n",
        "    right = left + crop_width\n",
        "    bottom = top + crop_height\n",
        "\n",
        "    pil_image = pil_image.crop((left, top, right, bottom))\n",
        "\n",
        "    # Basic Enhancements (can be AI-stylized in future)\n",
        "    enhancer = ImageEnhance.Contrast(pil_image)\n",
        "    pil_image = enhancer.enhance(1.1)\n",
        "\n",
        "    enhancer = ImageEnhance.Color(pil_image)\n",
        "    pil_image = enhancer.enhance(1.1)\n",
        "\n",
        "    pil_image = pil_image.filter(ImageFilter.SHARPEN)\n",
        "\n",
        "    return pil_image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Album-Level Vibe Caption Generation (Fully AI-powered) ---"
      ],
      "metadata": {
        "id": "FPaP5cDFN5-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlbumVibeCaptionGenerator:\n",
        "    def __init__(self):\n",
        "        print(\"\\nLoading AI models for captioning and vibe generation...\")\n",
        "        # BLIP for individual image descriptions (Vision-Encoder-Decoder)\n",
        "        self.blip_captioner = pipeline(\"image-to-text\", model=BLIP_MODEL_NAME, device=0 if torch.cuda.is_available() else -1)\n",
        "        print(f\"Loaded BLIP model: {BLIP_MODEL_NAME}\")\n",
        "\n",
        "        # Gemma for LLM-based album caption summarization and vibe generation\n",
        "        # Ensure you have accepted the model terms on Hugging Face and set up HF_TOKEN secret in Colab\n",
        "        try:\n",
        "            self.llm_tokenizer = AutoTokenizer.from_pretrained(GEMMA_LLM_MODEL)\n",
        "            self.llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "                GEMMA_LLM_MODEL,\n",
        "                torch_dtype=torch.float16, # Use float16 for memory efficiency\n",
        "                device_map=\"auto\", # Automatically map to GPU if available\n",
        "                token=os.environ.get(\"HF_TOKEN\") # Use HF_TOKEN from Colab Secrets\n",
        "            )\n",
        "            print(f\"Loaded LLM model: {GEMMA_LLM_MODEL}\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Could not load LLM model '{GEMMA_LLM_MODEL}'.\")\n",
        "            print(\"Please ensure you have accepted its terms on Hugging Face and set up your HF_TOKEN secret in Colab.\")\n",
        "            print(f\"Details: {e}\")\n",
        "            self.llm_tokenizer = None\n",
        "            self.llm_model = None\n",
        "\n",
        "    def generate_individual_captions(self, pil_images):\n",
        "        \"\"\"Generates captions for each image using the BLIP model.\"\"\"\n",
        "        individual_captions = []\n",
        "        if self.blip_captioner:\n",
        "            print(\"Generating individual image descriptions...\")\n",
        "            for i, img in enumerate(pil_images):\n",
        "                try:\n",
        "                    results = self.blip_captioner(img)\n",
        "                    if results:\n",
        "                        caption = results[0]['generated_text']\n",
        "                        individual_captions.append(f\"Image {i+1}: {caption}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error captioning image {i+1}: {e}\")\n",
        "                    individual_captions.append(f\"Image {i+1}: Could not describe this image.\")\n",
        "        return individual_captions\n",
        "\n",
        "    def generate_album_vibe_caption(self, individual_captions):\n",
        "        \"\"\"\n",
        "        Generates a single, vibe-fitting caption for the entire album using an LLM.\n",
        "        \"\"\"\n",
        "        if not self.llm_model:\n",
        "            return \"Could not generate album caption due to LLM loading error. Check Colab setup.\"\n",
        "\n",
        "        # Craft a prompt for the LLM based on the individual captions\n",
        "        context = \"\\n\".join(individual_captions)\n",
        "\n",
        "        prompt = textwrap.dedent(f\"\"\"\n",
        "        You are a highly creative and engaging social media caption generator for influencers.\n",
        "        You have analyzed a collection of photos from an album.\n",
        "        Below are individual descriptions of the key photos in the album.\n",
        "\n",
        "        Based on these descriptions, identify the overall theme, mood, and \"vibe\" of the entire album.\n",
        "        Then, generate a single, compelling, and short social media caption (max 3 sentences) that captures this vibe.\n",
        "        Include relevant emojis and 3-5 popular hashtags to maximize engagement.\n",
        "        Make it sound authentic and inspiring for social media influencers.\n",
        "\n",
        "        Individual photo descriptions:\n",
        "        ---\n",
        "        {context}\n",
        "        ---\n",
        "\n",
        "        Album Vibe Caption:\n",
        "        \"\"\")\n",
        "\n",
        "        # Tokenize and generate with the LLM\n",
        "\n",
        "        inputs = self.llm_tokenizer(prompt, return_tensors=\"pt\").to(self.llm_model.device)\n",
        "\n",
        "        outputs = self.llm_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100, # Adjust for desired length of the album caption\n",
        "            temperature=0.8,\n",
        "            do_sample=True,\n",
        "            pad_token_id=self.llm_tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        generated_text = self.llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        response_start = generated_text.find(\"Album Vibe Caption:\")\n",
        "        if response_start != -1:\n",
        "            generated_caption = generated_text[response_start + len(\"Album Vibe Caption:\"):].strip()\n",
        "        else:\n",
        "            generated_caption = generated_text\n",
        "\n",
        "        generated_caption = generated_caption.split(\"Individual photo descriptions:\")[0].strip()\n",
        "\n",
        "        if not any(tag.startswith('#') for tag in generated_caption.split()):\n",
        "            generated_caption += \"\\n#AlbumVibes #InfluencerLife #ContentCreator\"\n",
        "\n",
        "        return generated_caption"
      ],
      "metadata": {
        "id": "QXwsx3SyN621"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Main Processing Logic ---"
      ],
      "metadata": {
        "id": "BUWMKVbuOGNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_album(album_path):\n",
        "    if not os.path.exists(album_path):\n",
        "        print(f\"Error: Album path '{album_path}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    # Initialize AI models\n",
        "    album_caption_generator = AlbumVibeCaptionGenerator() # Handles both individual and album-level AI captioning\n",
        "    aesthetic_scorer = AestheticScorer() # AI aesthetic scoring (now a real model)\n",
        "\n",
        "    all_photos_info = [] # To store loaded images and their scores\n",
        "\n",
        "    # 1. Gather all photos and process Live Photos (AI-informed best shot)\n",
        "    photo_paths = get_image_paths(album_path)\n",
        "\n",
        "    if not photo_paths:\n",
        "        print(f\"No image files found in '{album_path}'. Please check the directory and file types.\")\n",
        "        return\n",
        "\n",
        "    for i, photo_entry in enumerate(photo_paths):\n",
        "        print(f\"[{i+1}/{len(photo_paths)}] Processing {photo_entry['type']} photo...\")\n",
        "        pil_image = None\n",
        "        original_path = \"\"\n",
        "\n",
        "        if photo_entry['type'] == 'live':\n",
        "            original_path = photo_entry['jpg_path']\n",
        "            best_frame = select_best_live_photo_frame(photo_entry['mov_path'])\n",
        "            if best_frame:\n",
        "                pil_image = best_frame\n",
        "                print(f\"Selected best frame from Live Photo: {os.path.basename(original_path)}\")\n",
        "            else:\n",
        "                print(f\"Could not process Live Photo: {os.path.basename(original_path)}. Skipping.\")\n",
        "                continue\n",
        "        else: # Still photo\n",
        "            original_path = photo_entry['path']\n",
        "            try:\n",
        "                pil_image = Image.open(original_path).convert('RGB')\n",
        "                print(f\"Successfully loaded still photo: {os.path.basename(original_path)}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error opening still photo {original_path}: {e}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "        if pil_image:\n",
        "            # Here's where the *real* AI aesthetic score is used\n",
        "            score = score_photo_engagement(pil_image, aesthetic_scorer)\n",
        "            all_photos_info.append({\n",
        "                'original_path': original_path,\n",
        "                'pil_image': pil_image,\n",
        "                'score': score\n",
        "            })\n",
        "            print(f\"  -> Scored {os.path.basename(original_path)}: {score:.2f} (AI Aesthetic Score)\")\n",
        "\n",
        "    # 2. Select top N photos based on AI-driven score\n",
        "    all_photos_info.sort(key=lambda x: x['score'], reverse=True)\n",
        "    selected_photos_info = all_photos_info[:MAX_PHOTOS_TO_SELECT]\n",
        "\n",
        "    if not selected_photos_info:\n",
        "        print(\"No suitable photos found or selected based on AI aesthetic and engagement scoring. Try adding more photos or adjusting MAX_PHOTOS_TO_SELECT.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nSelected {len(selected_photos_info)} best photos based on AI aesthetic and engagement scoring.\")\n",
        "\n",
        "    # Store individual processed images for album captioning\n",
        "    processed_pil_images_for_captioning = []\n",
        "\n",
        "    # 3. Process selected photos: edit, save\n",
        "    print(\"\\nApplying social media edits and saving selected photos...\")\n",
        "    for idx, photo_data in enumerate(selected_photos_info):\n",
        "        original_img_path = photo_data['original_path']\n",
        "        edited_pil_image = photo_data['pil_image'] # Start with original or best frame\n",
        "\n",
        "        # Apply edits (you can customize the aspect ratio here)\n",
        "        edited_pil_image = apply_social_media_edits(edited_pil_image, preferred_aspect_ratio=TARGET_ASPECT_RATIOS[0])\n",
        "\n",
        "        # Save edited image\n",
        "        output_filename = f\"post_{idx+1}_{os.path.basename(original_img_path)}\"\n",
        "        output_filepath = os.path.join(OUTPUT_DIR, output_filename)\n",
        "        edited_pil_image.save(output_filepath)\n",
        "        processed_pil_images_for_captioning.append(edited_pil_image) # Add to list for album captioning\n",
        "\n",
        "        print(f\"  Saved edited photo: {output_filepath}\")\n",
        "\n",
        "    # 4. Generate a single, vibe-fitting caption for the entire album (Fully AI-powered)\n",
        "    print(\"\\nGenerating album-level caption using LLM...\")\n",
        "    individual_blip_captions = album_caption_generator.generate_individual_captions(processed_pil_images_for_captioning)\n",
        "    album_vibe_caption = album_caption_generator.generate_album_vibe_caption(individual_blip_captions)\n",
        "\n",
        "    # Write the album caption to a file\n",
        "    album_caption_filepath = os.path.join(OUTPUT_DIR, \"album_vibe_caption.txt\")\n",
        "    with open(album_caption_filepath, 'w', encoding='utf-8') as f_album_caption:\n",
        "        f_album_caption.write(\"--- Album Vibe Caption for All Selected Photos ---\\n\\n\")\n",
        "        f_album_caption.write(textwrap.fill(album_vibe_caption, width=80)) # Wrap for readability\n",
        "        f_album_caption.write(\"\\n\\n--- Individual Photo Descriptions (for reference) ---\\n\\n\")\n",
        "        for cap in individual_blip_captions:\n",
        "            f_album_caption.write(textwrap.fill(cap, width=80) + \"\\n\")\n",
        "\n",
        "    print(f\"\\nAll processed photos saved to: {os.path.abspath(OUTPUT_DIR)}\")\n",
        "    print(f\"Album vibe caption and individual descriptions saved to: {os.path.abspath(album_caption_filepath)}\")\n",
        "    print(\"\\nProcessing Complete!\")"
      ],
      "metadata": {
        "id": "VPOl_aRFOGyj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}