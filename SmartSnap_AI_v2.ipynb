{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WomF1C_kOUXS"
      },
      "outputs": [],
      "source": [
        "!pip install Pillow opencv-python numpy transformers torch moviepy scikit-image dlib face_recognition accelerate bitsandbytes timm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For Gemma model access (if you haven't done this already):\n",
        "# 1. Go to https://huggingface.co/google/gemma-2b-it and accept its terms.\n",
        "# 2. Go to https://huggingface.co/settings/tokens to generate a new access token (e.g., 'gemma_access').\n",
        "# 3. In Colab, click the 'ðŸ”‘ Secrets' icon on the left sidebar.\n",
        "# 4. Add a new secret named 'HF_TOKEN' and paste your Hugging Face token as its value.\n",
        "# 5. Ensure the toggle next to 'HF_TOKEN' is ON for this notebook."
      ],
      "metadata": {
        "id": "UCN8xEVeOme7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "import cv2\n",
        "import numpy as np\n",
        "from moviepy.editor import VideoFileClip\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoFeatureExtractor, AutoModel # Import AutoFeatureExtractor, AutoModel\n",
        "import torch\n",
        "import face_recognition # For face detection (AI)\n",
        "import textwrap # For neatly wrapping LLM output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "DRduNpHgOi7i",
        "outputId": "5b61ae79-d103-46be-ad55-592d6d69355d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n",
            "Exception ignored in: <function _xla_gc_callback at 0x7d8acb287ba0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "    \n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'face_recognition'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cebe0b3673b1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoFeatureExtractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModel\u001b[0m \u001b[0;31m# Import AutoFeatureExtractor, AutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mface_recognition\u001b[0m \u001b[0;31m# For face detection (AI)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtextwrap\u001b[0m \u001b[0;31m# For neatly wrapping LLM output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'face_recognition'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Configuration ---"
      ],
      "metadata": {
        "id": "U4_o565ZOw27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR = \"processed_social_media_photos\"\n",
        "MAX_PHOTOS_TO_SELECT = 10 # Maximum photos to process and consider for the album vibe\n",
        "TARGET_ASPECT_RATIOS = [(1,1), (4,5)] # Preferred aspect ratios for social media (e.g., Instagram square, portrait)"
      ],
      "metadata": {
        "id": "_LkweD5DOraY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- AI Models Configuration ---"
      ],
      "metadata": {
        "id": "LpMMFN2YO780"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BLIP_MODEL_NAME = \"Salesforce/blip-image-captioning-base\" # For individual image descriptions (Vision-Encoder-Decoder AI)\n",
        "GEMMA_LLM_MODEL = \"google/gemma-2b-it\" # For album caption generation (Large Language Model AI)\n",
        "AESTHETIC_MODEL_NAME = \"HuggingFaceH4/aesthetic-predictor-v2\" # For aesthetic scoring (Dedicated AI model)"
      ],
      "metadata": {
        "id": "HJHgyb22O1Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- AI Aesthetic Scorer (Perfect AI Matrix Integration) ---"
      ],
      "metadata": {
        "id": "6vd7z_4KPH8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AestheticScorer:\n",
        "    def __init__(self):\n",
        "        print(\"\\nLoading AI Aesthetic Predictor Model...\")\n",
        "        self.processor = None\n",
        "        self.model = None\n",
        "        try:\n",
        "            # Load feature extractor (for preprocessing images) and the model\n",
        "            self.processor = AutoFeatureExtractor.from_pretrained(AESTHETIC_MODEL_NAME)\n",
        "            self.model = AutoModel.from_pretrained(AESTHETIC_MODEL_NAME)\n",
        "            # Move model to GPU if available\n",
        "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval() # Set model to evaluation mode\n",
        "            print(f\"Loaded AI Aesthetic Predictor: {AESTHETIC_MODEL_NAME} on {self.device}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Could not load AI Aesthetic Predictor model '{AESTHETIC_MODEL_NAME}'.\")\n",
        "            print(\"Please ensure you have all necessary libraries installed (e.g., 'timm').\")\n",
        "            print(f\"Details: {e}\")\n",
        "            self.processor = None\n",
        "            self.model = None\n",
        "            print(\"Aesthetic scoring will be skipped or may cause errors.\")\n",
        "\n",
        "    def predict_aesthetic_score(self, pil_image):\n",
        "        \"\"\"\n",
        "        Predicts an aesthetic score for an image using a pre-trained AI model.\n",
        "        The score typically ranges from 1 to 10.\n",
        "        \"\"\"\n",
        "        if self.processor is None or self.model is None:\n",
        "            # Fallback to a very basic heuristic or return a default score if model loading failed\n",
        "            print(\"Warning: Aesthetic predictor model not loaded. Using fallback score.\")\n",
        "            return 5.0 # Neutral score if AI model failed to load\n",
        "\n",
        "        try:\n",
        "            # Preprocess the image using the model's feature extractor\n",
        "            inputs = self.processor(images=pil_image, return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()} # Move inputs to device\n",
        "\n",
        "            # Perform inference\n",
        "            with torch.no_grad(): # Disable gradient calculation for inference\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            # The model's output is typically a single logit.\n",
        "            # Convert logit to a score, often scaled to 1-10.\n",
        "            # For HuggingFaceH4/aesthetic-predictor-v2, a common scaling is to interpret\n",
        "            # the single logit output as a direct score (e.g., 1-10 or 0-1).\n",
        "            # If the model outputs logits, a sigmoid or other activation might be needed.\n",
        "            # Based on common usage, a direct interpretation or clipping is often applied.\n",
        "            # Let's assume the output is a single float representing the score.\n",
        "            score = outputs.logits.squeeze().item() # Get the scalar value\n",
        "\n",
        "            # A common practice is to scale it if the model outputs raw logits or a different range.\n",
        "            # For this model, scores typically range from 1 to 10. We can clip for safety.\n",
        "            aesthetic_score = max(1.0, min(10.0, score)) # Ensure score is within 1 to 10 range\n",
        "\n",
        "            return aesthetic_score\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during aesthetic prediction for image: {e}\")\n",
        "            return 5.0 # Return a neutral score on prediction error\n"
      ],
      "metadata": {
        "id": "uNLbcHZsO_hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Utility Functions ---"
      ],
      "metadata": {
        "id": "qNbynFTtPhf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_paths(album_path):\n",
        "    \"\"\"Gathers all image and potential Live Photo paths.\"\"\"\n",
        "    image_extensions = ['jpg', 'jpeg', 'png', 'webp']\n",
        "    all_files = os.listdir(album_path)\n",
        "\n",
        "    photos = []\n",
        "    live_photos_map = {}\n",
        "\n",
        "    for filename in all_files:\n",
        "        name, ext = os.path.splitext(filename)\n",
        "        ext = ext.lower().lstrip('.')\n",
        "        full_path = os.path.join(album_path, filename)\n",
        "\n",
        "        if ext in image_extensions:\n",
        "            # Check if there's a corresponding .mov for Live Photos\n",
        "            if os.path.exists(os.path.join(album_path, name + '.mov')):\n",
        "                if name not in live_photos_map:\n",
        "                    live_photos_map[name] = {}\n",
        "                live_photos_map[name]['jpg'] = full_path\n",
        "            else:\n",
        "                photos.append({'type': 'still', 'path': full_path})\n",
        "        elif ext == 'mov':\n",
        "            if os.path.exists(os.path.join(album_path, name + '.jpg')):\n",
        "                if name not in live_photos_map:\n",
        "                    live_photos_map[name] = {}\n",
        "                live_photos_map[name]['mov'] = full_path\n",
        "\n",
        "    for name, files in live_photos_map.items():\n",
        "        if 'jpg' in files and 'mov' in files:\n",
        "            photos.append({'type': 'live', 'jpg_path': files['jpg'], 'mov_path': files['mov']})\n",
        "\n",
        "    return photos"
      ],
      "metadata": {
        "id": "TLW2A9fxPiTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Live Photo \"Best Shot\" Selection (AI-driven face analysis) ---"
      ],
      "metadata": {
        "id": "NCrHyZSzPqC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_blur(image_np):\n",
        "    \"\"\"Calculates a blur score for an image using Laplacian variance.\"\"\"\n",
        "    gray = cv2.cvtColor(image_np, cv2.COLOR_BGR2GRAY)\n",
        "    return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "\n",
        "def analyze_faces_for_quality(image_np):\n",
        "    \"\"\"\n",
        "    AI-driven Face Analysis:\n",
        "    Detects faces and evaluates quality factors like open eyes, smiles.\n",
        "    This is a conceptual placeholder; real implementations use ML models.\n",
        "    \"\"\"\n",
        "    face_locations = face_recognition.face_locations(image_np)\n",
        "    if not face_locations:\n",
        "        return 0, 0 # No faces, no face quality score\n",
        "\n",
        "    total_face_score = 0\n",
        "\n",
        "    for face_location in face_locations:\n",
        "        # A more advanced AI would detect specific expressions, eye closure, etc.\n",
        "        face_score = 100 # Base score for just having a face\n",
        "\n",
        "        # Dummy conditions for demonstration, replace with actual ML inference for expressions:\n",
        "        # e.g., using facial landmarks and a classifier for smiles/open eyes\n",
        "\n",
        "        total_face_score += face_score\n",
        "\n",
        "    return len(face_locations), total_face_score\n",
        "\n",
        "def select_best_live_photo_frame(mov_path):\n",
        "    \"\"\"\n",
        "    Extracts frames from a Live Photo video and selects the \"best\" one.\n",
        "    \"Best\" is determined by an AI-informed heuristic score based on blur and face quality.\n",
        "    \"\"\"\n",
        "    print(f\"Processing Live Photo: {mov_path}\")\n",
        "    best_frame = None\n",
        "    best_score = -1\n",
        "\n",
        "    try:\n",
        "        clip = VideoFileClip(mov_path)\n",
        "\n",
        "        # Process a limited number of frames to avoid excessive computation\n",
        "        frame_interval = max(1, int(clip.duration / 10)) # Get approx 10 frames\n",
        "\n",
        "        for i, frame_time in enumerate(np.arange(0, clip.duration, frame_interval)):\n",
        "            if i >= 20: # Limit total frames processed to 20\n",
        "                break\n",
        "            frame_np = clip.get_frame(frame_time) # RGB array\n",
        "\n",
        "            # Convert to BGR for OpenCV\n",
        "            frame_bgr = cv2.cvtColor(frame_np, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "            blur_score = detect_blur(frame_bgr)\n",
        "            num_faces, face_quality_score = analyze_faces_for_quality(frame_bgr)\n",
        "\n",
        "            # AI-informed scoring heuristic for best frame:\n",
        "            current_frame_score = blur_score * 0.5 + face_quality_score * 1.0\n",
        "\n",
        "            if current_frame_score > best_score:\n",
        "                best_score = current_frame_score\n",
        "                best_frame = frame_np\n",
        "\n",
        "        if best_frame is not None:\n",
        "            return Image.fromarray(best_frame)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing Live Photo {mov_path}: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "_oS7cEqJPqqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Image Quality & Engagement Scoring (AI-driven selection) ---"
      ],
      "metadata": {
        "id": "HGfoIsbpPxPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def score_photo_engagement(pil_image, aesthetic_scorer):\n",
        "    \"\"\"\n",
        "    Scores a photo based on aesthetic and engagement potential.\n",
        "    Leverages AI for aesthetic prediction.\n",
        "    \"\"\"\n",
        "    img_np = np.array(pil_image.convert('RGB'))\n",
        "    img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # 1. AI Aesthetic Score (Most important component for \"best shot\")\n",
        "    ai_aesthetic_score = aesthetic_scorer.predict_aesthetic_score(pil_image)\n",
        "\n",
        "    # 2. Face Presence & Quality Score (AI-informed)\n",
        "    num_faces, face_quality_score = analyze_faces_for_quality(img_bgr)\n",
        "\n",
        "    # 3. Basic Photography Metrics (supporting AI)\n",
        "    blur_score = detect_blur(img_bgr)\n",
        "    gray_image = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    brightness_mean = np.mean(gray_image)\n",
        "    exposure_deviation = abs(brightness_mean - 128)\n",
        "\n",
        "    # Combine scores with weights. Tune these weights!\n",
        "    # The AI aesthetic score is now a direct, powerful component.\n",
        "    total_score = (ai_aesthetic_score * 100) + \\\n",
        "                  (face_quality_score * 1.5) + \\\n",
        "                  (blur_score * 0.1) + \\\n",
        "                  (255 - exposure_deviation)\n",
        "\n",
        "    return total_score"
      ],
      "metadata": {
        "id": "XJW6pW4SP00V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Social Media-Ready Editing ---"
      ],
      "metadata": {
        "id": "p9oCBgpbP6ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_social_media_edits(pil_image, preferred_aspect_ratio=(4,5)):\n",
        "    \"\"\"Applies common social media edits: crop, enhance, sharpen.\"\"\"\n",
        "    img_width, img_height = pil_image.size\n",
        "\n",
        "    # Smart Cropping (AI for object-aware cropping is a future enhancement)\n",
        "    target_w, target_h = preferred_aspect_ratio\n",
        "\n",
        "    potential_h = int(img_width * (target_h / target_w))\n",
        "    if potential_h <= img_height:\n",
        "        crop_width = img_width\n",
        "        crop_height = potential_h\n",
        "        left = 0\n",
        "        top = (img_height - crop_height) // 2\n",
        "    else:\n",
        "        crop_height = img_height\n",
        "        crop_width = int(img_height * (target_w / target_h))\n",
        "        left = (img_width - crop_width) // 2\n",
        "        top = 0\n",
        "\n",
        "    right = left + crop_width\n",
        "    bottom = top + crop_height\n",
        "\n",
        "    pil_image = pil_image.crop((left, top, right, bottom))\n",
        "\n",
        "    # Basic Enhancements (can be AI-stylized in future)\n",
        "    enhancer = ImageEnhance.Contrast(pil_image)\n",
        "    pil_image = enhancer.enhance(1.1)\n",
        "\n",
        "    enhancer = ImageEnhance.Color(pil_image)\n",
        "    pil_image = enhancer.enhance(1.1)\n",
        "\n",
        "    pil_image = pil_image.filter(ImageFilter.SHARPEN)\n",
        "\n",
        "    return pil_image"
      ],
      "metadata": {
        "id": "WnSukFrTP7yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Album-Level Vibe Caption Generation (Fully AI-powered) ---"
      ],
      "metadata": {
        "id": "CvW56AYXQGcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlbumVibeCaptionGenerator:\n",
        "    def __init__(self):\n",
        "        print(\"\\nLoading AI models for captioning and vibe generation...\")\n",
        "        # BLIP for individual image descriptions (Vision-Encoder-Decoder)\n",
        "        self.blip_captioner = pipeline(\"image-to-text\", model=BLIP_MODEL_NAME, device=0 if torch.cuda.is_available() else -1)\n",
        "        print(f\"Loaded BLIP model: {BLIP_MODEL_NAME}\")\n",
        "\n",
        "        # Gemma for LLM-based album caption summarization and vibe generation\n",
        "        # Ensure you have accepted the model terms on Hugging Face and set up HF_TOKEN secret in Colab\n",
        "        try:\n",
        "            self.llm_tokenizer = AutoTokenizer.from_pretrained(GEMMA_LLM_MODEL)\n",
        "            self.llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "                GEMMA_LLM_MODEL,\n",
        "                torch_dtype=torch.float16, # Use float16 for memory efficiency\n",
        "                device_map=\"auto\", # Automatically map to GPU if available\n",
        "                token=os.environ.get(\"HF_TOKEN\") # Use HF_TOKEN from Colab Secrets\n",
        "            )\n",
        "            print(f\"Loaded LLM model: {GEMMA_LLM_MODEL}\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Could not load LLM model '{GEMMA_LLM_MODEL}'.\")\n",
        "            print(\"Please ensure you have accepted its terms on Hugging Face and set up your HF_TOKEN secret in Colab.\")\n",
        "            print(f\"Details: {e}\")\n",
        "            self.llm_tokenizer = None\n",
        "            self.llm_model = None\n",
        "\n",
        "    def generate_individual_captions(self, pil_images):\n",
        "        \"\"\"Generates captions for each image using the BLIP model.\"\"\"\n",
        "        individual_captions = []\n",
        "        if self.blip_captioner:\n",
        "            print(\"Generating individual image descriptions...\")\n",
        "            for i, img in enumerate(pil_images):\n",
        "                try:\n",
        "                    results = self.blip_captioner(img)\n",
        "                    if results:\n",
        "                        caption = results[0]['generated_text']\n",
        "                        individual_captions.append(f\"Image {i+1}: {caption}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error captioning image {i+1}: {e}\")\n",
        "                    individual_captions.append(f\"Image {i+1}: Could not describe this image.\")\n",
        "        return individual_captions\n",
        "\n",
        "    def generate_album_vibe_caption(self, individual_captions):\n",
        "        \"\"\"\n",
        "        Generates a single, vibe-fitting caption for the entire album using an LLM.\n",
        "        \"\"\"\n",
        "        if not self.llm_model:\n",
        "            return \"Could not generate album caption due to LLM loading error. Check Colab setup.\"\n",
        "\n",
        "        # Craft a prompt for the LLM based on the individual captions\n",
        "        context = \"\\n\".join(individual_captions)\n",
        "\n",
        "        prompt = textwrap.dedent(f\"\"\"\n",
        "        You are a highly creative and engaging social media caption generator for influencers.\n",
        "        You have analyzed a collection of photos from an album.\n",
        "        Below are individual descriptions of the key photos in the album.\n",
        "\n",
        "        Based on these descriptions, identify the overall theme, mood, and \"vibe\" of the entire album.\n",
        "        Then, generate a single, compelling, and short social media caption (max 3 sentences) that captures this vibe.\n",
        "        Include relevant emojis and 3-5 popular hashtags to maximize engagement.\n",
        "        Make it sound authentic and inspiring for social media influencers.\n",
        "\n",
        "        Individual photo descriptions:\n",
        "        ---\n",
        "        {context}\n",
        "        ---\n",
        "\n",
        "        Album Vibe Caption:\n",
        "        \"\"\")\n",
        "\n",
        "        # Tokenize and generate with the LLM\n",
        "\n",
        "        inputs = self.llm_tokenizer(prompt, return_tensors=\"pt\").to(self.llm_model.device)\n",
        "\n",
        "        outputs = self.llm_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100, # Adjust for desired length of the album caption\n",
        "            temperature=0.8,\n",
        "            do_sample=True,\n",
        "            pad_token_id=self.llm_tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        generated_text = self.llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        response_start = generated_text.find(\"Album Vibe Caption:\")\n",
        "        if response_start != -1:\n",
        "            generated_caption = generated_text[response_start + len(\"Album Vibe Caption:\"):].strip()\n",
        "        else:\n",
        "            generated_caption = generated_text\n",
        "\n",
        "        generated_caption = generated_caption.split(\"Individual photo descriptions:\")[0].strip()\n",
        "\n",
        "        if not any(tag.startswith('#') for tag in generated_caption.split()):\n",
        "            generated_caption += \"\\n#AlbumVibes #InfluencerLife #ContentCreator\"\n",
        "\n",
        "        return generated_caption"
      ],
      "metadata": {
        "id": "1nCIkREOQHOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Main Processing Logic ---"
      ],
      "metadata": {
        "id": "mzod5sqkQRV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_album(album_path):\n",
        "    if not os.path.exists(album_path):\n",
        "        print(f\"Error: Album path '{album_path}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    # Initialize AI models\n",
        "    album_caption_generator = AlbumVibeCaptionGenerator() # Handles both individual and album-level AI captioning\n",
        "    aesthetic_scorer = AestheticScorer() # AI aesthetic scoring (now a real model)\n",
        "\n",
        "    all_photos_info = [] # To store loaded images and their scores\n",
        "\n",
        "    # 1. Gather all photos and process Live Photos (AI-informed best shot)\n",
        "    photo_paths = get_image_paths(album_path)\n",
        "\n",
        "    if not photo_paths:\n",
        "        print(f\"No image files found in '{album_path}'. Please check the directory and file types.\")\n",
        "        return\n",
        "\n",
        "    for i, photo_entry in enumerate(photo_paths):\n",
        "        print(f\"[{i+1}/{len(photo_paths)}] Processing {photo_entry['type']} photo...\")\n",
        "        pil_image = None\n",
        "        original_path = \"\"\n",
        "\n",
        "        if photo_entry['type'] == 'live':\n",
        "            original_path = photo_entry['jpg_path']\n",
        "            best_frame = select_best_live_photo_frame(photo_entry['mov_path'])\n",
        "            if best_frame:\n",
        "                pil_image = best_frame\n",
        "                print(f\"Selected best frame from Live Photo: {os.path.basename(original_path)}\")\n",
        "            else:\n",
        "                print(f\"Could not process Live Photo: {os.path.basename(original_path)}. Skipping.\")\n",
        "                continue\n",
        "        else: # Still photo\n",
        "            original_path = photo_entry['path']\n",
        "            try:\n",
        "                pil_image = Image.open(original_path).convert('RGB')\n",
        "                print(f\"Successfully loaded still photo: {os.path.basename(original_path)}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error opening still photo {original_path}: {e}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "        if pil_image:\n",
        "            # Here's where the *real* AI aesthetic score is used\n",
        "            score = score_photo_engagement(pil_image, aesthetic_scorer)\n",
        "            all_photos_info.append({\n",
        "                'original_path': original_path,\n",
        "                'pil_image': pil_image,\n",
        "                'score': score\n",
        "            })\n",
        "            print(f\"  -> Scored {os.path.basename(original_path)}: {score:.2f} (AI Aesthetic Score)\")\n",
        "\n",
        "    # 2. Select top N photos based on AI-driven score\n",
        "    all_photos_info.sort(key=lambda x: x['score'], reverse=True)\n",
        "    selected_photos_info = all_photos_info[:MAX_PHOTOS_TO_SELECT]\n",
        "\n",
        "    if not selected_photos_info:\n",
        "        print(\"No suitable photos found or selected based on AI aesthetic and engagement scoring. Try adding more photos or adjusting MAX_PHOTOS_TO_SELECT.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nSelected {len(selected_photos_info)} best photos based on AI aesthetic and engagement scoring.\")\n",
        "\n",
        "    # Store individual processed images for album captioning\n",
        "    processed_pil_images_for_captioning = []\n",
        "\n",
        "    # 3. Process selected photos: edit, save\n",
        "    print(\"\\nApplying social media edits and saving selected photos...\")\n",
        "    for idx, photo_data in enumerate(selected_photos_info):\n",
        "        original_img_path = photo_data['original_path']\n",
        "        edited_pil_image = photo_data['pil_image'] # Start with original or best frame\n",
        "\n",
        "        # Apply edits (you can customize the aspect ratio here)\n",
        "        edited_pil_image = apply_social_media_edits(edited_pil_image, preferred_aspect_ratio=TARGET_ASPECT_RATIOS[0])\n",
        "\n",
        "        # Save edited image\n",
        "        output_filename = f\"post_{idx+1}_{os.path.basename(original_img_path)}\"\n",
        "        output_filepath = os.path.join(OUTPUT_DIR, output_filename)\n",
        "        edited_pil_image.save(output_filepath)\n",
        "        processed_pil_images_for_captioning.append(edited_pil_image) # Add to list for album captioning\n",
        "\n",
        "        print(f\"  Saved edited photo: {output_filepath}\")\n",
        "\n",
        "    # 4. Generate a single, vibe-fitting caption for the entire album (Fully AI-powered)\n",
        "    print(\"\\nGenerating album-level caption using LLM...\")\n",
        "    individual_blip_captions = album_caption_generator.generate_individual_captions(processed_pil_images_for_captioning)\n",
        "    album_vibe_caption = album_caption_generator.generate_album_vibe_caption(individual_blip_captions)\n",
        "\n",
        "    # Write the album caption to a file\n",
        "    album_caption_filepath = os.path.join(OUTPUT_DIR, \"album_vibe_caption.txt\")\n",
        "    with open(album_caption_filepath, 'w', encoding='utf-8') as f_album_caption:\n",
        "        f_album_caption.write(\"--- Album Vibe Caption for All Selected Photos ---\\n\\n\")\n",
        "        f_album_caption.write(textwrap.fill(album_vibe_caption, width=80)) # Wrap for readability\n",
        "        f_album_caption.write(\"\\n\\n--- Individual Photo Descriptions (for reference) ---\\n\\n\")\n",
        "        for cap in individual_blip_captions:\n",
        "            f_album_caption.write(textwrap.fill(cap, width=80) + \"\\n\")\n",
        "\n",
        "    print(f\"\\nAll processed photos saved to: {os.path.abspath(OUTPUT_DIR)}\")\n",
        "    print(f\"Album vibe caption and individual descriptions saved to: {os.path.abspath(album_caption_filepath)}\")\n",
        "    print(\"\\nProcessing Complete!\")"
      ],
      "metadata": {
        "id": "ifztpttbQSII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Colab Specific Execution Block ---"
      ],
      "metadata": {
        "id": "puvB28E4QagD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    from google.colab import drive\n",
        "    import os\n",
        "\n",
        "    # IMPORTANT: Please accept Gemma's terms and set up your HF_TOKEN in Colab Secrets\n",
        "    # (See instructions at the top of this code block)\n",
        "\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Define the path to your photo album in Google Drive.\n",
        "    # *** IMPORTANT: Change this to the ACTUAL path where you uploaded your photos! ***\n",
        "    # Example: If your folder is named 'my_influencer_photos' directly in MyDrive:\n",
        "    album_directory = \"/content/drive/MyDrive/my_influencer_photos\"\n",
        "\n",
        "    # --- Colab Setup Instructions ---\n",
        "    print(\"\\n--- Colab Setup Reminder ---\")\n",
        "    print(\"1. Ensure you have run the `!pip install ...` cell at the top.\")\n",
        "    print(\"2. Ensure you have accepted Gemma's terms on Hugging Face AND the terms for 'HuggingFaceH4/aesthetic-predictor-v2' \")\n",
        "    print(\"   if prompted, and added your HF_TOKEN to Colab Secrets.\")\n",
        "    print(\"3. Upload your photo album folder to your Google Drive, and update 'album_directory' above to its exact path.\")\n",
        "    print(f\"   Currently set to: {album_directory}\")\n",
        "    print(\"\\nStarting AI photo processing...\")\n",
        "    # -------------------------------\n",
        "\n",
        "    process_album(album_directory)"
      ],
      "metadata": {
        "id": "G7Gm66dIQbMx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}